{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c24df62-11a3-492e-9be4-9faef2950108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 16:33:32.743344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 16:33:33.010264: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-16 16:33:33.068052: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-16 16:33:33.749156: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-16 16:33:33.749250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-16 16:33:33.749258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import seaborn as sns\n",
    "from ExplainableMaximumLikelihoodCalculator import ExplainableMaximumLikelihoodCalculator\n",
    "import pysam\n",
    "from Bio import SeqIO, Seq, SeqRecord, pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from LoadDataUtils import getListOfReadsFromBamFile, getListOfReadsFromFastaFile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import shap\n",
    "from scipy.stats import chisquare\n",
    "from scipy.special import rel_entr\n",
    "fileNameSapiens =  \"data/simulatedData/human_chinese_AF346973_500samples.fas\" #File that contains 500 reads from homo sapiens\n",
    "fileNameNeanderthals =  \"data/simulatedData/Neanderthal_Goyet_KX198085_500samples.fas\" #File that contains 500 reads from neanderthal\n",
    "fileNameDenisovans =  \"data/simulatedData/denisova_kx663333_500samples.fas\" #File that contains 500 reads from denisovan\n",
    "neanderthals_500_generated = getListOfReadsFromFastaFile(fileNameNeanderthals)\n",
    "sapiens_500_generated = getListOfReadsFromFastaFile(fileNameSapiens)\n",
    "denisovan_500_samples = getListOfReadsFromFastaFile(fileNameDenisovans)\n",
    "path_to_frequencies_table = \"data/substitution_matrix.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed0b0af-06c0-4734-bf32-f145015350ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sapiens_reference_file_names = [\n",
    "                    \"data/reference_files/human_AF346981_French.fa\",\n",
    "                     \"data/reference_files/human_AY195760_Korea.fa\",\n",
    "                      \"data/reference_files/human_AY882416_Ethiopia.fa\",\n",
    "                      \"data/reference_files/human_AY963586_Italian.fa\",\n",
    "                      \"data/reference_files/human_AY195781_Caucasian.fa\",\n",
    "                      \"data/reference_files/human_AY195757_Iraqi-Israeli.fa\",\n",
    "                      \"data/reference_files/human_AY195749_NativeAmerican.fa\"]\n",
    "neanderthals_reference_file_names = [\n",
    "                            \"data/reference_files/neanderthal_mezmaiskaya1_FM865411.fa\",\n",
    "                           \"data/reference_files/Neanderthal_Altai_KC879692.fa\",\n",
    "                           \"data/reference_files/Neanderthal_Denisova11_full_mtDNA_KU131206.fa\",\n",
    "                           \"data/reference_files/Neanderthal_Spy_94a_MG025538.fa\",\n",
    "                            \"data/reference_files/Neanderthal_Vindija33.16_AM948965.fa\",\n",
    "                            \"data/reference_files/Neanderthal_Vindija33.19_KJ533545.fa\"]\n",
    "denisovan_reference_file_names = [  \n",
    "                        \"data/reference_files/Denisova_MT576653.1.fa\",\n",
    "                        \"data/reference_files/Denisova_MT576652.1.fa\",\n",
    "                        \"data/reference_files/Denisova_4_FR695060.fa\",\n",
    "                        \"data/reference_files/Denisova_8_KT780370.fa\",\n",
    "                        \"data/reference_files/Denisova_manual_phalanx_NC_013993.fa\",\n",
    "                        \"data/reference_files/Denisova_MT576651.1.fa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9bb5cc-2b73-44b3-87e1-3e5bc7094ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(sap, nean, den):\n",
    "    list_before =  list(np.random.choice(sapiens_500_generated, sap)) + list(np.random.choice(neanderthals_500_generated, nean)) + list(np.random.choice(denisovan_500_samples, den))\n",
    "    l = []\n",
    "    for i in list_before:\n",
    "        l.append(str(i))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fc5573-72da-4653-abb1-301fb9646702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def find_all_subsets(group):\n",
    "    all_subsets = []\n",
    "    n = len(group)\n",
    "    \n",
    "    for r in range(n + 1):\n",
    "        subsets_r = combinations(group, r)\n",
    "        all_subsets.extend(list(subsets_r))\n",
    "        \n",
    "    return all_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa91ea8-ce61-4875-b35e-881a254b9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shapley_values_for_datum(data_point, length_of_data, ml_calculator):\n",
    "    indexes_without_datum = [i for i in range(length_of_data) if i != data_point]\n",
    "    all_subsets = find_all_subsets(indexes_without_datum)\n",
    "    sum_over_all_subsets = 0 \n",
    "    for G in all_subsets:\n",
    "        g_with = [i for i in G]\n",
    "        g_with.append(data_point)\n",
    "        with_datum = ml_calculator.calc_maximum_likelihood_on_subset(subset_of_indexes=g_with).values[0]\n",
    "        without_datum = ml_calculator.calc_maximum_likelihood_on_subset(subset_of_indexes=G).values[0]\n",
    "        with_minus_without = with_datum - without_datum\n",
    "        multiplier = np.math.factorial(len(G)) * np.math.factorial(length_of_data - len(G) - 1)\n",
    "        value_for_sum = with_minus_without * multiplier \n",
    "        sum_over_all_subsets += value_for_sum\n",
    "    shapleys = sum_over_all_subsets / np.math.factorial(length_of_data)\n",
    "    return  shapleys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3129ad63-3c4f-4c36-9c82-88b26a14c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l_1_distance(vector_a, vector_b):\n",
    "    a = np.array(vector_a)\n",
    "    b = np.array(vector_b)\n",
    "    return np.linalg.norm((a - b), ord=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d19519f-78ac-414b-a743-993802cde256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrderedValues(values, data_len):\n",
    "    ordered_values = []\n",
    "    for i in range(data_len):\n",
    "        shapley_of_datum = [k[0][i] for k in values]\n",
    "        ordered_values.append(shapley_of_datum)\n",
    "    return ordered_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0674e294-5eed-4499-9f8a-1334899a653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_of_shap_and_mc_to_ground_truth_shapleys(data):\n",
    "    maximum_likelihood_calculator_d_1 = ExplainableMaximumLikelihoodCalculator(data,\n",
    "                                                    ref_neanderthal_file_names=neanderthals_reference_file_names,\n",
    "                                                    ref_sapien_file_names=sapiens_reference_file_names,\n",
    "                                                    ref_denisovan_file_names=denisovan_reference_file_names,\n",
    "                                                    path_to_substitution_matrix=path_to_frequencies_table,\n",
    "                                                    number_of_jobs=-1)\n",
    "    ground_truth_shapleys = []\n",
    "    for i in range(len(data)):\n",
    "        ground_truth_shapleys.append(calculate_shapley_values_for_datum(i,len(data),maximum_likelihood_calculator_d_1))\n",
    "    shaps = maximum_likelihood_calculator_d_1.calculate_shapley_values()\n",
    "    shaps_ordered = getOrderedValues(shaps, len(data))\n",
    "    shapley_values_monte_carlo = maximum_likelihood_calculator_d_1.estimate_shapley_values(number_of_samples_per_read = 20)[1] # for the not scaled values\n",
    "    mc_ordered = getOrderedValues(shapley_values_monte_carlo, len(data))\n",
    "    sum_of_distances_shap = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_of_distances_shap += calculate_l_1_distance(shaps_ordered[i], ground_truth_shapleys[i])\n",
    "    sum_of_distances_mc = 0\n",
    "    for i in range(len(data)):\n",
    "        sum_of_distances_mc += calculate_l_1_distance(mc_ordered[i], ground_truth_shapleys[i])\n",
    "    return (sum_of_distances_shap, sum_of_distances_mc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc22a483-fbe9-4ba8-bbab-a24af50e1302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mLoading sequences and calculating alignments to all references, this might take a while. Number of reads: \u001b[0m 8\n",
      "start working on read number 0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d6c05e193c4f6b889737c27eddd892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "254\n",
      "\u001b[32mStart working on read number 0 in processId 1073\u001b[0m\n",
      "\u001b[32mStart working on read number 1 in processId 1074\u001b[0m\n",
      "\u001b[32mStart working on read number 2 in processId 1075\u001b[0m\n",
      "\u001b[32mStart working on read number 3 in processId 1076\u001b[0m\u001b[32mStart working on read number 4 in processId 1077\u001b[0m\n",
      "\n",
      "\u001b[32mStart working on read number 5 in processId 1078\u001b[0m\n",
      "\u001b[32mStart working on read number 7 in processId 1080\u001b[0m\u001b[32mStart working on read number 6 in processId 1079\u001b[0m\n",
      "\n",
      "(0.27064285714285596, 1.1517380952380953)\n",
      "\u001b[34mLoading sequences and calculating alignments to all references, this might take a while. Number of reads: \u001b[0m 8\n",
      "start working on read number 0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd1c0a92c6c45b2b417c05014022d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "254\n",
      "\u001b[32mStart working on read number 0 in processId 1164\u001b[0m\n",
      "\u001b[32mStart working on read number 1 in processId 1165\u001b[0m\n",
      "\u001b[32mStart working on read number 2 in processId 1166\u001b[0m\n",
      "\u001b[32mStart working on read number 3 in processId 1167\u001b[0m\u001b[32mStart working on read number 4 in processId 1168\u001b[0m\n",
      "\u001b[32mStart working on read number 6 in processId 1170\u001b[0m\u001b[32mStart working on read number 7 in processId 1171\u001b[0m\u001b[32mStart working on read number 5 in processId 1169\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "(0.2597976190476204, 1.659428571428571)\n",
      "\u001b[34mLoading sequences and calculating alignments to all references, this might take a while. Number of reads: \u001b[0m 8\n",
      "start working on read number 0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e6daf9353b4ed4a7648f94e8e9941e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "254\n",
      "\u001b[32mStart working on read number 0 in processId 1255\u001b[0m\n",
      "\u001b[32mStart working on read number 1 in processId 1256\u001b[0m\n",
      "\u001b[32mStart working on read number 2 in processId 1257\u001b[0m\n",
      "\u001b[32mStart working on read number 3 in processId 1258\u001b[0m\n",
      "\u001b[32mStart working on read number 5 in processId 1260\u001b[0m\u001b[32mStart working on read number 4 in processId 1259\u001b[0m\u001b[32mStart working on read number 6 in processId 1261\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mStart working on read number 7 in processId 1262\u001b[0m\n",
      "(0.23052380952380921, 1.6210119047619038)\n"
     ]
    }
   ],
   "source": [
    "sapiens = 2\n",
    "neanderthals = 4\n",
    "denisovans = 2\n",
    "results = []\n",
    "for i in range(3):\n",
    "    dataset = generate_sample(sapiens, neanderthals, denisovans)\n",
    "    distance = calculate_distance_of_shap_and_mc_to_ground_truth_shapleys(data=dataset)\n",
    "    results.append(distance)\n",
    "    print(distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a801d799-9363-4c1b-ad80-0a22d6e58113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.27064285714285596, 1.1517380952380953),\n",
       " (0.2597976190476204, 1.659428571428571),\n",
       " (0.23052380952380921, 1.6210119047619038)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37c60b-6741-440e-9729-f21770c34d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
